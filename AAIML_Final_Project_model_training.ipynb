{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqJY8OCUSEXj",
        "outputId": "08558941-d956-4112-d98a-6bdabd313af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxyaSG9GQrPr",
        "outputId": "b8e9fcdb-3637-4dd7-b75f-066c799ba5ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version THIS session is using: 2.0.2\n",
            "Mounted at /content/drive\n",
            "Inspecting arrays inside: /content/drive/MyDrive/AAI511_ML/preprocessed_composer_data_v2.npz\n",
            "\n",
            "--> Array: 'network_input'\n",
            "    Shape: (6742025, 100, 1)\n",
            "    Data Type (dtype): float64\n",
            "--------------------\n",
            "--> Array: 'network_output'\n",
            "    Shape: (6742025,)\n",
            "    Data Type (dtype): int64\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "print(f\"NumPy version THIS session is using: {np.__version__}\")\n",
        "import os\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_path = '/content/drive/MyDrive/AAI511_ML/preprocessed_composer_data_v2.npz'\n",
        "\n",
        "print(f\"Inspecting arrays inside: {save_path}\\n\")\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(save_path) as zf:\n",
        "        for name in zf.namelist():\n",
        "            if not name.endswith('.npy'):\n",
        "                continue\n",
        "\n",
        "            with zf.open(name) as fp:\n",
        "                # Read the magic string to get the file format version\n",
        "                version = np.lib.format.read_magic(fp)\n",
        "\n",
        "                # Call the correct header function based on the version\n",
        "                if version[0] == 1:\n",
        "                    shape, fortran_order, dtype = np.lib.format.read_array_header_1_0(fp)\n",
        "                elif version[0] == 2:\n",
        "                    shape, fortran_order, dtype = np.lib.format.read_array_header_2_0(fp)\n",
        "                else:\n",
        "                    print(f\"--> Array: '{name.replace('.npy', '')}' has an unsupported format version: {version}\")\n",
        "                    continue\n",
        "\n",
        "                array_name = name.replace('.npy', '')\n",
        "                print(f\"--> Array: '{array_name}'\")\n",
        "                print(f\"    Shape: {shape}\")\n",
        "                print(f\"    Data Type (dtype): {dtype}\")\n",
        "                print(\"-\" * 20)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while inspecting the file: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def create_tf_dataset(file_path, batch_size=32):\n",
        "    \"\"\"Create a TensorFlow dataset that loads data efficiently\"\"\"\n",
        "\n",
        "    # Load with memory mapping\n",
        "    data = np.load(file_path, mmap_mode='r')\n",
        "    network_input = data['network_input']\n",
        "    network_output = data['network_output']\n",
        "\n",
        "    # Create TensorFlow dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((network_input, network_output))\n",
        "\n",
        "    # Optimize the pipeline\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)  # Prefetch next batch while training\n",
        "    dataset = dataset.cache()  # Cache in memory if possible\n",
        "\n",
        "    return dataset, data\n",
        "\n",
        "# Your rewritten loading code\n",
        "if os.path.exists(save_path):\n",
        "    print(f\"Loading preprocessed data from {save_path}...\")\n",
        "\n",
        "    # Create TensorFlow dataset (this replaces the old loading method)\n",
        "    train_dataset, data_file = create_tf_dataset(save_path, batch_size=32)\n",
        "\n",
        "    # Get shapes and info from the memory-mapped data\n",
        "    network_input = data_file['network_input']\n",
        "    network_output = data_file['network_output']\n",
        "\n",
        "    print(f\"Data loaded successfully!\")\n",
        "    print(f\"Network input shape: {network_input.shape}\")\n",
        "    print(f\"Network output shape: {network_output.shape}\")\n",
        "    print(f\"Network input dtype: {network_input.dtype}\")\n",
        "    print(f\"Network output dtype: {network_output.dtype}\")\n",
        "\n",
        "    # The data_file stays open for the dataset to use\n",
        "    print(f\"TensorFlow dataset created with batch size: 64\")\n",
        "    print(f\"Number of batches: {len(train_dataset)}\")\n",
        "\n",
        "else:\n",
        "    print(f\"File not found at {save_path}\")\n",
        "    print(\"Please check the file path or re-run your preprocessing code.\")\n",
        "\n",
        "# Now use train_dataset in your model training instead of network_input/network_output\n",
        "print(\"\\n✓ Use 'train_dataset' in your model.fit() instead of network_input/network_output\")\n",
        "print(\"Example: model.fit(train_dataset, epochs=10)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92iVbhslUipk",
        "outputId": "602a1b9c-de8b-4c80-f9a6-aef7fbfd59c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading preprocessed data from /content/drive/MyDrive/AAI511_ML/preprocessed_composer_data_v2.npz...\n",
            "Data loaded successfully!\n",
            "Network input shape: (6742025, 100, 1)\n",
            "Network output shape: (6742025,)\n",
            "Network input dtype: float64\n",
            "Network output dtype: int64\n",
            "TensorFlow dataset created with batch size: 64\n",
            "Number of batches: 210689\n",
            "\n",
            "✓ Use 'train_dataset' in your model.fit() instead of network_input/network_output\n",
            "Example: model.fit(train_dataset, epochs=10)\n"
          ]
        }
      ]
    }
  ]
}